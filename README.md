#  Samsung EnnovateX 2025 AI Challenge Submission  

- **Problem Statement** â€“ #2 Building the Untethered, Always-On AI Companion 
- **Team name** â€“ IdeaFlow  
- **Team members** â€“ Nitheeswaran M, Sree Ram Roshan A S  
- **Demo Video Link** â€“ [YouTube Link](https://youtu.be/JQ_UYD9q5aM?si=h1Um2VMUCo4pI-Dj)

---


# NOVA â€“ Offline AI Companion with Vision & Voice

<p align="center">
  <img src="docs/nova_logo.jpeg" alt="NOVA Logo" width="120"/>
</p>

<p align="center">
  <b>Offline-first AI assistant for Android</b><br>
  Voice | Vision | Chat â€” all on-device, no cloud required.
</p>

---

## ğŸ“– Overview

**NOVA** is an Android application that brings together:

- ğŸ—£ï¸ **Offline Speech Recognition** (Vosk)  
- ğŸ’¬ **On-device LLM inference** (MediaPipe GenAI `.task` bundles)  
- ğŸ“· **Real-time Object Detection** (TensorFlow Lite EfficientDet)  
- ğŸ”Š **Text-to-Speech** (Android TTS)  

Unlike most assistants that rely on cloud APIs, NOVA is **offline-first**, designed for privacy, low-latency responses, and accessibility in areas without reliable internet.  
Think of it as a **ChatGPT-style experience** + **computer vision awareness** â€” but entirely on-device.  

---

## âœ¨ Features

- **Conversational Chat UI** inspired by ChatGPT  
- **Wake words**: â€œHey Novaâ€, â€œOk Novaâ€  
- **Camera Vision Mode**: describes objects (*â€œI see a laptopâ€*)  
- **Offline operation**: all models run locally  
- **Voice feedback** with natural Text-to-Speech  
- **Material Design**: app bar, watermark branding, chat bubbles  

<p align="center">
  <img src="docs/screenshot_chat.jpeg" alt="Chat UI" width="250"/>
  <img src="docs/screenshot_vision.jpeg" alt="Vision Mode" width="250"/>
</p>

---

## ğŸ—ï¸ Architecture

<p align="center">
  <img src="docs/NOVA_Architecture.jpeg" alt="Architecture Diagram" width="600"/>
</p>

**Data Flow:**

```
Conversation:
User Speech â†’ Vosk ASR â†’ LLM (MediaPipe GenAI) â†’ TTS + Chat UI

Vision:
Camera â†’ TensorFlow Lite Detector â†’ NOVA Response â†’ TTS + Overlay
```

---

## ğŸ“‚ Project Structure

```
NOVA-main/
â”‚â”€â”€ app/
â”‚   â”œâ”€â”€ src/main/java/com/example/nova/
â”‚   â”‚   â”œâ”€â”€ MainActivity.kt        # Core activity: chat + vision
â”‚   â”‚   â”œâ”€â”€ MyVoskService.java     # Vosk integration
â”‚   â”‚   â”œâ”€â”€ llm/LlmEngine.kt       # MediaPipe LLM wrapper
â”‚   â”‚   â”œâ”€â”€ ObjectDetector.kt      # TFLite object detection
â”‚   â”‚   â”œâ”€â”€ CommandRouter.kt       # Intent routing
â”‚   â”‚   â””â”€â”€ util/ZipSanity.kt      # Model bundle validator
â”‚   â”œâ”€â”€ src/main/res/layout/activity_main.xml
â”‚   â”œâ”€â”€ build.gradle.kts
â”‚   â””â”€â”€ ...
â”œâ”€â”€ build.gradle.kts
â”œâ”€â”€ settings.gradle.kts
â”œâ”€â”€ README.md
```

---

## âš™ï¸ Setup & Installation

### Prerequisites
- Android Studio **Arctic Fox (2020.3.1)** or newer  
- Android SDK 24+  
- Gradle KTS  

### Steps

1. **Clone the repo**
   ```bash
   git clone https://github.com/your-username/NOVA.git
   cd NOVA-main
   ```

2. **Open in Android Studio**  
   Sync Gradle and let dependencies install.

3. **Build & Run**
   ```bash
   ./gradlew assembleDebug
   adb install app/build/outputs/apk/debug/app-debug.apk
   ```

---

## ğŸ“¦ Model Download & Setup

The Gemma `.task` model is **not included in this repository** because of GitHubâ€™s file size limits.  
Instead, download it directly from **Hugging Face**:

### ğŸ”— Download Link
- [Gemma3-1B-IT `.task` model (Q4, EKV2048)](https://huggingface.co/litert-community/Gemma3-1B-IT/blob/main/Gemma3-1B-IT_multi-prefill-seq_q4_ekv2048.task)

### ğŸ“‚ Where to Place the Model
After downloading:

1. Copy the file to your **phoneâ€™s Downloads folder**:
   ```
   /storage/emulated/0/Download/Gemma3-1B-IT_multi-prefill-seq_q4_ekv2048.task
   ```

2. Open the **NOVA app** â†’ when prompted, **import the model**.  
   The app will validate the `.task` file and move it into its **private storage**:
   ```
   /data/data/com.example.nova/files/models/Gemma3-1B-IT_multi-prefill-seq_q4_ekv2048.task
   ```

3. Once imported, youâ€™ll see:
   ```
   Model imported: Gemma3-1B-IT_multi-prefill-seq_q4_ekv2048.task (xxxx MB). Say somethingâ€¦
   ```

âœ… From now on, the app will **auto-load** the Gemma model at startup.

âš ï¸ **Note on performance**:  
- First-time warm-up may take **30â€“60 seconds** (model graph compilation).  
- Runs fully **offline** after that.  
- Works best on devices with **6GB+ RAM**.

---

## ğŸ§‘â€ğŸ’» Tech Stack

- **Kotlin + Java** â€“ core development  
- **Material Design Components** â€“ UI  
- **Vosk Android** â€“ offline ASR  
- **MediaPipe Tasks GenAI** â€“ LLM inference  
- **TensorFlow Lite** â€“ object detection  
- **Android TTS** â€“ voice feedback  
- **Gradle (KTS)** â€“ build system  
- **Min SDK:** 24  
- **Target SDK:** 34  

---

## ğŸš€ Roadmap

- [ ] Hybrid cloud fallback (if internet available)  
- [ ] Persistent conversation memory  
- [ ] Multilingual speech + text support  
- [ ] Optimized models for AR/wearables  

---

## ğŸ¤ Contributing

Contributions, issues, and feature requests are welcome!  
Please check the [issues page](https://github.com/your-username/NOVA/issues).  

---

## ğŸ“œ License

Distributed under the MIT License. See `LICENSE` for details.  

---

## ğŸ™Œ Acknowledgements

- [Vosk](https://alphacephei.com/vosk/)  
- [MediaPipe](https://developers.google.com/mediapipe/)  
- [TensorFlow Lite](https://www.tensorflow.org/lite)  
- [Hugging Face â€“ Gemma3-1B-IT](https://huggingface.co/litert-community/Gemma3-1B-IT)  
